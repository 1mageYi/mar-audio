# inference_v2.py
import torch
import argparse
import os
from transformers import T5Tokenizer, T5EncoderModel

# ç¡®ä¿ä½ çš„æ¨¡å‹å’ŒVAE wrapperçš„å¯¼å…¥è·¯å¾„æ˜¯æ­£ç¡®çš„
from models.mar_audio import mar_audio_large # ä½¿ç”¨V2æ¨¡å‹
from models.vae_audio import VAEWrapper
import util.misc as misc

def get_args_parser():
    parser = argparse.ArgumentParser('V2 Inference Script', add_help=False)
    # --- æ¨¡å‹å’Œè·¯å¾„ ---
    parser.add_argument('--model_name', default='mar_audio_large', type=str, help='Name of the model architecture')
    parser.add_argument('--checkpoint_path', required=True, type=str, help='Path to the model checkpoint (.pth file)')
    parser.add_argument('--vae_config_path', default="configs/stable_audio_2_0_vae.json", type=str)
    parser.add_argument('--vae_ckpt_path', default="pretrained_models/vae/stable_audio_open_vae_weights.pth", type=str)
    parser.add_argument('--tokenizer_path', default='t5-base', type=str)
    
    # --- ç”Ÿæˆå‚æ•° ---
    parser.add_argument('--prompt', required=True, type=str, help='Text prompt for audio generation')
    parser.add_argument('--cfg_scale', default=7.0, type=float, help='Classifier-Free Guidance scale')
    parser.add_argument('--num_iter', default=100, type=int, help='Number of generation steps')
    parser.add_argument('--temperature', default=1.0, type=float, help='Sampling temperature')
    parser.add_argument('--diffloss_d', type=int, default=12, help='Depth of the Diffusion Loss MLP')
    parser.add_argument('--diffloss_w', type=int, default=1536, help='Width of the Diffusion Loss MLP')
    #ã€‘
    # --- è¾“å‡ºå’Œè®¾å¤‡ ---
    parser.add_argument('--output_path', default='generated_audio_v2.wav', type=str, help='Path to save the generated audio')
    parser.add_argument('--device', default='cuda', type=str)
    
    return parser

@torch.no_grad()
def main(args):
    print("--- V2 Model Inference ---")
    device = torch.device(args.device)

    # 1. åŠ è½½æ‰€æœ‰éœ€è¦çš„æ¨¡å‹
    print("åŠ è½½æ¨¡å‹...")
    vae = VAEWrapper(args.vae_config_path, args.vae_ckpt_path, device=device)
    tokenizer = T5Tokenizer.from_pretrained(args.tokenizer_path)
    text_encoder = T5EncoderModel.from_pretrained(args.tokenizer_path).to(device).eval()

    # åŠ è½½ä½ çš„MARæ¨¡å‹
    model = mar_audio_large(
        text_feature_dim=text_encoder.config.d_model,
        diffloss_d=args.diffloss_d,
        diffloss_w=args.diffloss_w
    ).to(device).eval()

    # ä»æ£€æŸ¥ç‚¹åŠ è½½æƒé‡
    print(f"ä»æ£€æŸ¥ç‚¹åŠ è½½æƒé‡: {args.checkpoint_path}")
    checkpoint = torch.load(args.checkpoint_path, map_location='cpu', weights_only=False)
    
    # æˆ‘ä»¬ä½¿ç”¨EMAæƒé‡è¿›è¡Œæ¨ç†ï¼Œå› ä¸ºå®ƒä»¬é€šå¸¸æ•ˆæœæ›´å¥½
    ema_state_dict = checkpoint['model_ema']
    model.load_state_dict(ema_state_dict)

    # 2. å‡†å¤‡è¾“å…¥
    print(f"å‡†å¤‡Prompt: '{args.prompt}'")
    tokens = tokenizer(args.prompt, padding='max_length', truncation=True, max_length=128, return_tensors='pt').input_ids.to(device)
    text_features = text_encoder(tokens).last_hidden_state.mean(dim=1) # V2ä½¿ç”¨å¹³å‡æ± åŒ–

    # 3. æ‰§è¡Œç”Ÿæˆ
    print(f"å¼€å§‹ç”Ÿæˆ... (CFG Scale: {args.cfg_scale}, Steps: {args.num_iter})")
    sampled_latents = model.sample_tokens(
        text_features=text_features,
        num_iter=args.num_iter,
        cfg_scale=args.cfg_scale,
        temperature=args.temperature,
        progress=True
    )
    
    # 4. è§£ç å¹¶ä¿å­˜
    print(f"è§£ç å¹¶ä¿å­˜è‡³: {args.output_path}")
    vae.decode_batch(sampled_latents, [args.output_path])
    
    print("\nğŸ‰ ç”Ÿæˆå®Œæˆï¼ğŸ‰")

if __name__ == '__main__':
    args = get_args_parser().parse_args()
    main(args)