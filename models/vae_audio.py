import torch
import torchaudio
import json
from stable_audio_tools.models.factory import create_model_from_config
from stable_audio_tools.inference.utils import prepare_audio
import numpy as np


class VAEWrapper:
    """
    ä¸€ä¸ªç”¨äº Stable Audio VAE æ¨¡å‹çš„å°è£…ç±»ï¼Œè´Ÿè´£åŠ è½½æ¨¡å‹ã€
    ç¼–ç éŸ³é¢‘æ–‡ä»¶ä¸ºæ½œåœ¨å‘é‡ï¼ˆlatentsï¼‰ï¼Œä»¥åŠä»æ½œåœ¨å‘é‡è§£ç å›éŸ³é¢‘ã€‚
    (å·²æ›´æ–°ä»¥æ”¯æŒæ‰¹é‡å¤„ç†)
    """

    def __init__(self, model_config_path, model_ckpt_path, target_length=441000, device=None):
        """
        åˆå§‹åŒ–å¹¶åŠ è½½ VAE æ¨¡å‹ã€‚
        """
        print("æ­£åœ¨åˆå§‹åŒ– VAE Wrapper...")
        
        if device is None:
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = device
        
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")

        # --- åŠ è½½æ¨¡å‹é…ç½® ---
        with open(model_config_path) as f:
            self.model_config = json.load(f)
        self.target_sample_rate = self.model_config["sample_rate"]
        self.chunk_size = self.model_config["sample_size"]
        self.target_channels = self.model_config.get("audio_channels", 2)
        self.target_length = target_length

        # --- åˆ›å»ºå¹¶åŠ è½½æ¨¡å‹ ---
        print("æ­£åœ¨åˆ›å»ºå¹¶åŠ è½½ VAE æ¨¡å‹...")
        self.model = create_model_from_config(self.model_config)
        
        state_dict = torch.load(model_ckpt_path, map_location=self.device)
        self.model.load_state_dict(state_dict)
        
        self.model.to(self.device).eval()


        print("VAE æ¨¡å‹åŠ è½½å®Œæˆã€‚")

    def preprocess_audio(self, audio_path):
        """
        åŠ è½½å¹¶é¢„å¤„ç†å•ä¸ªéŸ³é¢‘æ–‡ä»¶ã€‚
        """
        try:
            audio, sample_rate = torchaudio.load(audio_path)
            audio = prepare_audio(audio,
                                  in_sr=sample_rate,
                                  target_sr=self.target_sample_rate,
                                  target_length=self.target_length,
                                  target_channels=self.target_channels,
                                  device=self.device,
                                  )
            return audio.squeeze(0) 
        except Exception as e:
            print(f"å¤„ç†éŸ³é¢‘æ–‡ä»¶ {audio_path} æ—¶å‡ºé”™: {e}. è¿”å›é™éŸ³ã€‚")
            return torch.zeros((self.model_config.get("audio_channels", 2), self.target_length)).to(self.device)

    def encode(self, input_audio_path):
        """
        å°†ã€å•ä¸ªã€‘éŸ³é¢‘æ–‡ä»¶ç¼–ç ä¸ºæ½œåœ¨å‘é‡ã€‚
        """
        audio_tensor = self.preprocess_audio(input_audio_path).unsqueeze(0)
        with torch.no_grad():
            # self.model.encode_audio æœŸæœ›ä¸€ä¸ª batch, æ‰€ä»¥æˆ‘ä»¬å¢åŠ ä¸€ä¸ªç»´åº¦
            latents = self.model.encode_audio(audio_tensor, chunk_size=self.chunk_size)
        return latents.permute(0, 2, 1) # è¿”å› [B, L, D] = [1, 215, 64]

    def decode(self, latents, output_audio_path):
        """
        å°†ã€å•ä¸ªã€‘æ½œåœ¨å‘é‡è§£ç ä¸ºéŸ³é¢‘æ–‡ä»¶ã€‚
        """
        # å°† [1, L, D] è½¬å›æ¨¡å‹æœŸæœ›çš„ [1, D, L]
        latents_for_decode = latents.permute(0, 2, 1)
        with torch.no_grad():
            reconstructed_audio = self.model.decode_audio(latents_for_decode, chunk_size=self.chunk_size)
        reconstructed_audio = reconstructed_audio.squeeze(0)
        torchaudio.save(output_audio_path, reconstructed_audio.cpu(), self.target_sample_rate)
        
    # --- æ–°å¢çš„æ‰¹é‡å¤„ç†æ–¹æ³• ---
    def encode_batch(self, audio_paths: list):
        """
        å°†ã€ä¸€æ‰¹ã€‘éŸ³é¢‘æ–‡ä»¶ç¼–ç ä¸ºæ½œåœ¨å‘é‡ã€‚
        """
        # preprocess_audio è¿”å› 2D å¼ é‡ [channels, length]
        audio_tensors = [self.preprocess_audio(path) for path in audio_paths]
        # torch.stack ä¼šå°†ä¸€åˆ— [channels, length] çš„å¼ é‡æ­£ç¡®å †å ä¸º [batch, channels, length]
        audio_batch = torch.stack(audio_tensors, dim=0)

        with torch.no_grad():
            latents_batch = self.model.encode_audio(audio_batch, chunk_size=self.chunk_size)
        
        return latents_batch.permute(0, 2, 1)

    def decode_batch(self, latents_batch, output_paths: list):
        """
        å°†ã€ä¸€æ‰¹ã€‘æ½œåœ¨å‘é‡è§£ç ä¸ºå¤šä¸ªéŸ³é¢‘æ–‡ä»¶ã€‚
        """
        if len(latents_batch) != len(output_paths):
            raise ValueError("æ½œåœ¨å‘é‡æ‰¹æ¬¡å¤§å°å¿…é¡»ä¸è¾“å‡ºè·¯å¾„åˆ—è¡¨é•¿åº¦åŒ¹é…ã€‚")

        latents_for_decode = latents_batch.permute(0, 2, 1)
        with torch.no_grad():
            reconstructed_audio_batch = self.model.decode_audio(latents_for_decode, chunk_size=self.chunk_size)
        
        for i in range(reconstructed_audio_batch.shape[0]):
            audio_sample = reconstructed_audio_batch[i].cpu()
            torchaudio.save(output_paths[i], audio_sample, self.target_sample_rate)

if __name__ == "__main__":
    model_config_path = "configs/stable_audio_2_0_vae.json"
    model_ckpt_path = "pretrained_models/vae/stable_audio_open_vae_weights.pth"
    
    # å‡è®¾æˆ‘ä»¬æœ‰è¿™äº›æ–‡ä»¶
    input_audios = ["/root/data1/yimingjing/workspace/stable-audio-tools/output.wav", "/root/data1/yimingjing/workspace/stable-audio-tools/diffusers_test_10s.wav"]
    output_audios = ["test/reconstructed1.wav", "test/reconstructed2.wav"]
    
    try:
        vae = VAEWrapper(model_config_path, model_ckpt_path)
        
        print("\n--- æµ‹è¯•æ‰¹é‡ç¼–ç  ---")
        latents_batch = vae.encode_batch(input_audios)
        print(f"å¾—åˆ°çš„ Latent æ‰¹æ¬¡å½¢çŠ¶: {latents_batch.shape}")

        print("\n--- æµ‹è¯•æ‰¹é‡è§£ç  ---")
        vae.decode_batch(latents_batch, output_audios)
        print(f"è§£ç å®Œæˆï¼ŒéŸ³é¢‘å·²ä¿å­˜è‡³ {output_audios}")
        
        print("\nğŸ‰ æ‰¹é‡å¤„ç†æµç¨‹æˆåŠŸå®Œæˆï¼ğŸ‰")

    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")