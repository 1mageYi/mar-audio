import torch
import torchaudio
import json
from stable_audio_tools.models.factory import create_model_from_config
from stable_audio_tools.inference.utils import prepare_audio



class VAEWrapper:
    """
    ä¸€ä¸ªç”¨äº Stable Audio VAE æ¨¡å‹çš„å°è£…ç±»ï¼Œè´Ÿè´£åŠ è½½æ¨¡å‹ã€
    ç¼–ç éŸ³é¢‘æ–‡ä»¶ä¸ºæ½œåœ¨å‘é‡ï¼ˆlatentsï¼‰ï¼Œä»¥åŠä»æ½œåœ¨å‘é‡è§£ç å›éŸ³é¢‘ã€‚
    """

    def __init__(self, model_config_path, model_ckpt_path, device=None):
        """
        åˆå§‹åŒ–å¹¶åŠ è½½ VAE æ¨¡å‹ã€‚

        å‚æ•°:
            model_config_path (str): VAE æ¨¡å‹é…ç½® .json æ–‡ä»¶çš„è·¯å¾„ã€‚
            model_ckpt_path (str): é¢„è®­ç»ƒçš„ VAE æƒé‡ .pth æˆ– .ckpt æ–‡ä»¶çš„è·¯å¾„ã€‚
            device (torch.device, å¯é€‰): æŒ‡å®šè¿è¡Œæ¨¡å‹çš„è®¾å¤‡ (ä¾‹å¦‚ 'cuda', 'cpu')ã€‚
                                         å¦‚æœä¸º Noneï¼Œåˆ™è‡ªåŠ¨æ£€æµ‹ GPUã€‚
        """
        print("æ­£åœ¨åˆå§‹åŒ– VAE Wrapper...")
        
        if device is None:
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = device
        
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")

        # --- åŠ è½½æ¨¡å‹é…ç½® ---
        with open(model_config_path) as f:
            self.model_config = json.load(f)
        self.target_sample_rate = self.model_config["sample_rate"]
        self.target_channels = self.model_config.get("audio_channels", 2)
        self.chunk_size = self.model_config["sample_size"]

        # --- åˆ›å»ºå¹¶åŠ è½½æ¨¡å‹ ---
        print("æ­£åœ¨åˆ›å»ºå¹¶åŠ è½½ VAE æ¨¡å‹...")
        self.model = create_model_from_config(self.model_config)
        
        state_dict = torch.load(model_ckpt_path, map_location=self.device)
        self.model.load_state_dict(state_dict)
        
        self.model.to(self.device).eval()
        print("VAE æ¨¡å‹åŠ è½½å®Œæˆã€‚")

    def preprocess_audio(self, audio_path, target_length=441000):
        """
        åŠ è½½å¹¶é¢„å¤„ç†éŸ³é¢‘æ–‡ä»¶ä»¥åŒ¹é…æ¨¡å‹è¾“å…¥è¦æ±‚ã€‚

        å‚æ•°:
            audio_path (str): è¾“å…¥éŸ³é¢‘æ–‡ä»¶çš„è·¯å¾„ã€‚
        
        è¿”å›:
            torch.Tensor: é¢„å¤„ç†åçš„éŸ³é¢‘å¼ é‡ï¼Œå½¢çŠ¶ä¸º [1, channels, samples]ã€‚
        """
        audio, sample_rate = torchaudio.load(audio_path)

        # é‡é‡‡æ ·
        audio = prepare_audio(audio,
                              in_sr=sample_rate,
                              target_sr=self.target_sample_rate,
                              target_length=target_length)
        
        # ç§»åŠ¨åˆ°è®¾å¤‡å¹¶å¢åŠ  batch ç»´åº¦
        return audio.to(self.device).unsqueeze(0)

    def encode(self, input_audio_path):
        """
        å°†éŸ³é¢‘æ–‡ä»¶ç¼–ç ä¸ºæ½œåœ¨å‘é‡ã€‚

        å‚æ•°:
            input_audio_path (str): è¾“å…¥éŸ³é¢‘æ–‡ä»¶çš„è·¯å¾„ã€‚

        è¿”å›:
            torch.Tensor: ç¼–ç åçš„æ½œåœ¨å‘é‡å¼ é‡ã€‚
        """
        print(f"æ­£åœ¨å¯¹ '{input_audio_path}' è¿›è¡Œç¼–ç ...")
        audio_tensor = self.preprocess_audio(input_audio_path)
        
        print(f"é¢„å¤„ç†åéŸ³é¢‘å½¢çŠ¶: {audio_tensor.shape}")
        
        with torch.no_grad():
            latents = self.model.encode_audio(
                audio_tensor, 
                chunk_size=self.chunk_size, 
            )
        print("ç¼–ç å®Œæˆã€‚")
        return latents

    def decode(self, latents, output_audio_path):
        """
        å°†æ½œåœ¨å‘é‡è§£ç ä¸ºéŸ³é¢‘æ–‡ä»¶ã€‚

        å‚æ•°:
            latents (torch.Tensor): ä» encode æ–¹æ³•å¾—åˆ°çš„æ½œåœ¨å‘é‡å¼ é‡ã€‚
            output_audio_path (str): ä¿å­˜è§£ç åéŸ³é¢‘çš„è·¯å¾„ã€‚
        """
        print(f"æ­£åœ¨è§£ç æ½œåœ¨å‘é‡...")
        with torch.no_grad():
            reconstructed_audio = self.model.decode_audio(
                latents, 
                chunk_size=self.chunk_size, 
            )
        
        # ç§»é™¤ batch ç»´åº¦å¹¶ä¿å­˜æ–‡ä»¶
        reconstructed_audio = reconstructed_audio.squeeze(0)
        torchaudio.save(output_audio_path, reconstructed_audio.cpu(), self.target_sample_rate)
        print(f"è§£ç å®Œæˆï¼ŒéŸ³é¢‘å·²ä¿å­˜è‡³ '{output_audio_path}'")


def main():
    """
    ä¸»å‡½æ•°ï¼Œç”¨äºæ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ VAEWrapper ç±»ã€‚
    """
    # --- è¯·ä¿®æ”¹è¿™é‡Œçš„è·¯å¾„ ---
    model_config_path = "path/to/your/stable_audio_1_0_vae.json"
    model_ckpt_path = "path/to/your/stable_audio_open_vae_weights.pth"
    input_audio = "path/to/your/input_audio.wav"
    output_audio = "path/to/your/reconstructed_audio.wav"

    try:
        # 1. åˆå§‹åŒ–å°è£…ç±»
        vae = VAEWrapper(model_config_path, model_ckpt_path)
        
        # 2. ç¼–ç éŸ³é¢‘æ–‡ä»¶
        latents = vae.encode(input_audio)
        
        # æ‰“å°ä¸€ä¸‹ latent çš„å½¢çŠ¶ï¼Œä»¥ä¾›å‚è€ƒ
        print(f"å¾—åˆ°çš„ Latent å½¢çŠ¶: {latents.shape}")

        # 3. ä» latent è§£ç å›éŸ³é¢‘
        vae.decode(latents, output_audio)
        
        print("\nğŸ‰ éŸ³é¢‘é‡å»ºæµç¨‹æˆåŠŸå®Œæˆï¼ğŸ‰")

    except FileNotFoundError:
        print("\nâŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ã€‚è¯·ç¡®ä¿ä½ å·²ç»æ­£ç¡®è®¾ç½®äº† `model_config_path`, `model_ckpt_path` å’Œ `input_audio` çš„è·¯å¾„ã€‚")
    except Exception as e:
        print(f"\nâŒ å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")


if __name__ == "__main__":
    main()